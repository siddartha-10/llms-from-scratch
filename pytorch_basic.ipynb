{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__) # to check the pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available()) # to check if the mps is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# creating pytorch tensors\n",
    "\n",
    "tensor0d = torch.tensor(1)\n",
    "print(tensor0d)\n",
    "\n",
    "tensor1d = torch.tensor([1,2,3])\n",
    "print(tensor1d)\n",
    "\n",
    "tensor2d = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(tensor2d)\n",
    "\n",
    "tensor3d = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(tensor3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the dataype of the tensor\n",
    "tensor0d.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_tensor = torch.tensor([1.0,2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# when the values are int then the default type is always torch.int64, when the values are float the default is torch.float32\n",
    "print(float_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# to change the datatype we use .to function\n",
    "tensor0d_float = tensor0d.to(torch.float32)\n",
    "print(tensor0d_float.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the shape we use .shape\n",
    "tensor2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to change the shape of the tensor we .reshape and we can also use the .view\n",
    "tensor2d_reshaped = tensor2d.reshape([3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal matrix\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "transposed matrix\n",
      "tensor([[1, 3],\n",
      "        [2, 4]])\n"
     ]
    }
   ],
   "source": [
    "# .T is used to get the transpose of a matrix\n",
    "m1 = torch.tensor([[1,2],[3,4]])\n",
    "m2 = m1.T\n",
    "\n",
    "print(\"Orginal matrix\")\n",
    "print(m1)\n",
    "print(\"transposed matrix\")\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 14],\n",
      "        [14, 20]]) matrix multiplication using matmul\n",
      "tensor([[10, 14],\n",
      "        [14, 20]]) matrix multiplication using @\n"
     ]
    }
   ],
   "source": [
    "# to do matrix multiplication we can use .matmul and also @\n",
    "m3 = m2.matmul(m1)\n",
    "m4 = m2@m1\n",
    "print(m3, \"matrix multiplication using matmul\")\n",
    "print(m4, \"matrix multiplication using @\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Gradients in Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "z = w1*x1 + b\n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a,y)\n",
    "\n",
    "gradient_loss_w1 = grad(loss,w1,retain_graph=True)\n",
    "gradient_loss_b = grad(loss,b,retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)  This is the gradient of loss W.R.T w1\n",
      "(tensor([-0.0817]),)  This is the gradient of loss W.R.T b\n"
     ]
    }
   ],
   "source": [
    "print(gradient_loss_w1, \" This is the gradient of loss W.R.T w1\")\n",
    "print(gradient_loss_b, \" This is the gradient of loss W.R.T b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above process we have done everything manually it is useful for debugging but pytroch has something very simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "z = w1*x1 + b\n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a,y)\n",
    "\n",
    "loss.backward()\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code implementing a classic multilayer perceptron with two hidden layers.\n",
    "\n",
    "1) We use the torch.nn.Module to build our own architecture.\n",
    "2) We use the init constructor to define the network layers and forward method to see how the inputs pass and interact.\n",
    "3) We use the .backwards method to inside of the training loop to calculate the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            torch.nn.Linear(20, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(50,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model) # this is used to see the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2213\n"
     ]
    }
   ],
   "source": [
    "# To check the number of trainable parameter's\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0058,  0.1188,  0.1084,  ..., -0.0280, -0.1022,  0.0612],\n",
      "        [ 0.0535,  0.0830, -0.0996,  ...,  0.1353, -0.0698,  0.1284],\n",
      "        [ 0.1357,  0.0864,  0.0097,  ..., -0.1301, -0.0637,  0.0754],\n",
      "        ...,\n",
      "        [-0.0945,  0.0780,  0.1143,  ...,  0.0452, -0.1234, -0.1341],\n",
      "        [ 0.0315, -0.1160, -0.0495,  ...,  0.0477,  0.0805,  0.0192],\n",
      "        [ 0.0871, -0.0545,  0.1396,  ..., -0.0317, -0.1230,  0.0414]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# NeuralNetwork(\n",
    "#   (layers): Sequential(\n",
    "#     (0): Linear(in_features=50, out_features=30, bias=True)\n",
    "#     (1): ReLU()\n",
    "#     (2): Linear(in_features=30, out_features=20, bias=True)\n",
    "#     (3): ReLU()\n",
    "#     (4): Linear(in_features=20, out_features=3, bias=True)\n",
    "#   )\n",
    "# )\n",
    "\n",
    "# To access the paramerters for any layer the above network \n",
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 50])\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# For reproducability purposes we can use manual_seed\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(50,3)\n",
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1670,  0.1001, -0.1219]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((1,50))\n",
    "out = model(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1670,  0.1001, -0.1219]])\n"
     ]
    }
   ],
   "source": [
    "# why we use this during inference is because with grad it will slow down the process. This tells pytroch to not keep track of the \n",
    "# gradients hence faster inference.\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2983, 0.3896, 0.3121]])\n"
     ]
    }
   ],
   "source": [
    "# we used softmax to get the class membership\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(x), dim=1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Efficient Data Loaders\n",
    "1) The custom dataset class is used to instantiate objects that define how each data record is loaded\n",
    "2) The dataloader class is used to assemble and shuffle the data into batches etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a toy dataset\n",
    "x_train = torch.tensor([\n",
    "    [-1.2,3.1],\n",
    "    [-0.9,2.9],\n",
    "    [-0.5,2.6],\n",
    "    [2.3,-1.1],\n",
    "    [2.7,-1.5]\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0,0,0,1,1])\n",
    "\n",
    "x_test = torch.tensor([\n",
    "    [-0.8,2.8],\n",
    "    [2.6,-1.6]\n",
    "])\n",
    "\n",
    "y_test = torch.tensor([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.features = x\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.features[index]\n",
    "        return one_x, one_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "train_ds = ToyDataset(x_train, y_train)\n",
    "test_ds = ToyDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
