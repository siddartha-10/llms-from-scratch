{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e68a86-4816-44a0-a07c-a9c20c38e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df0dfa2-0d29-420b-8c6f-b75251c58cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1af7574-ae6d-4b60-ba17-1b8d8c31f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0 \n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.context_length = context_length\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(self.context_length, self.context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.w_key(x) # Applying key matrix\n",
    "        values = self.w_value(x) # Applying value matrix\n",
    "        queries = self.w_query(x)  # Applying query matrix\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) # Reshaping keys so we can apply attention to all the batches at once rather than splitting them and then applying attention\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim) # Reshaping values so we can apply attention to all the batches at once rather than splitting them and then applying attention\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim) # Reshaping queries so we can apply attention to all the batches at once rather than splitting them and then applying attention\n",
    "\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(2, 3) # Calculating attention scores\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # Applying -inf to the upper triangular matrix so softmax will zero out the values\n",
    "        attention_scores = attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attention_weights = torch.softmax(attention_scores / (keys.shape[-1]**0.5), dim=-1) # Applying softmax\n",
    "        attention_weights = self.dropout(attention_weights) # Applying dropout\n",
    "        context_vector = (attention_weights @ values).transpose(1, 2) # Calculating context vector\n",
    "        context_vector = context_vector.contiguous().view(b, num_tokens, self.d_out) # Reshaping context vector\n",
    "        context_vector = self.out_proj(context_vector) # Applying output projection layer\n",
    "        return context_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d9788b1-5f83-49dc-a4fd-608f5ffa8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim=True)\n",
    "        var = x.var(dim = -1, keepdim=True, unbiased=False)\n",
    "        out_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale*out_norm + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a80847d-c763-4d33-a455-b882a5335f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], 4*cfg['emb_dim']),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*cfg['emb_dim'], cfg['emb_dim']),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba5bab1-51dd-4587-98e7-cfeaec8bd824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_in = cfg['emb_dim'],\n",
    "            d_out = cfg['emb_dim'],\n",
    "            dropout = cfg['dropout_rate'],\n",
    "            num_heads = cfg['num_heads'],\n",
    "            qkv_bias = cfg['qkv_bias'],\n",
    "            context_length = cfg['context_length']\n",
    "        )\n",
    "        self.FeedForward = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['dropout_rate'])\n",
    "\n",
    "    def forward(self,x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.FeedForward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27426857-b913-4db8-b2ef-5dfd54717955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['dropout_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias = False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "721e8135-9433-49ac-b356-720fe3b0dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    'vocab_size': 50257,\n",
    "    'emb_dim': 768,\n",
    "    'context_length': 256,\n",
    "    'n_layers': 12,\n",
    "    'num_heads': 12,\n",
    "    'dropout_rate': 0.1,\n",
    "    'qkv_bias': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ff0e478-2624-4ffa-851a-f568e289a513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2c5d08f-3770-40a1-a270-8f274240174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa544d65-a4e8-4ba1-b293-56a61880aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## text_to_token_ids and token_ids_to_text\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # adding the batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # removing the batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e446beb-b9c3-40b3-8edc-5e748688b68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: \n",
      " Every effort moves you 960esame WindsorFE Keith awaitedSer GaelListMine\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "start_content = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx = text_to_token_ids(start_content, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "\n",
    "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c995b-5bcc-4299-a60d-73acb97a6b1d",
   "metadata": {},
   "source": [
    "### Calculating the text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "086158ba-b691-434e-9f2d-b965ba3b9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"every effort moves\"\n",
    "text2 = \"I really like\"\n",
    "target1 = \" effort moves you\"\n",
    "target2 = \" really like chocolate\"\n",
    "\n",
    "t1_tensor = text_to_token_ids(text1, tokenizer)\n",
    "t2_tensor = text_to_token_ids(text2, tokenizer)\n",
    "\n",
    "t3_tensor = text_to_token_ids(target1, tokenizer)\n",
    "t4_tensor = text_to_token_ids(target2, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0175a01a-7c4b-4b16-91be-feb893568bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16833,  3626,  6100]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bcf675f-f843-4f48-adbe-092385afb1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  40, 1107,  588]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b607eb68-2538-47c2-9ab9-620d534aee1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3626, 6100,  345]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cffcb5a-e18e-4b84-9d1d-502778707cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1107,   588, 11311]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a5af1f6-9e47-4a04-9ddd-8a1250d1e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([\n",
    "    [16833,  3626,  6100],\n",
    "    [40, 1107,  588]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5e49318-f119-4c11-8929-9409b9e08ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([\n",
    "    [3626, 6100, 345],\n",
    "    [588, 428, 11311]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "934f0987-55ba-44a0-80a6-536031215826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65f88734-1a43-49df-b993-c0b2939370b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: \n",
      " tensor([[[13207],\n",
      "         [  552],\n",
      "         [42826]],\n",
      "\n",
      "        [[18236],\n",
      "         [34817],\n",
      "         [ 7055]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs: \\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7caa056f-a307-478c-a29c-a1ed8127c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_b1 = token_ids_to_text(targets[0], tokenizer)\n",
    "tatget_b1_model = token_ids_to_text(token_ids[0].flatten(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "031bcb91-484d-40a9-a56d-4f1432f1e0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " effort moves you\n",
      "hole compNetflix\n"
     ]
    }
   ],
   "source": [
    "print(target_b1)\n",
    "print(tatget_b1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e719055-79a6-4a42-8f06-4e3a03f459ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0,1,2], targets[text_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74ba9e39-c715-470f-86f6-3c2c88bcec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0,1,2], targets[text_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d654178d-82cc-4f48-8593-1db1c1dcfb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.0549e-05, 2.7952e-05, 8.2801e-06]) \n",
      " tensor([1.7212e-05, 1.5561e-05, 5.2184e-06])\n"
     ]
    }
   ],
   "source": [
    "print(target_probas_1, \"\\n\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f7c031c-94ab-45ce-a8d9-9033a114afc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.8926, -10.4850, -11.7017, -10.9699, -11.0707, -12.1633])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9a45542-a387-4649-a587-9631c0444728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-11.0472)\n"
     ]
    }
   ],
   "source": [
    "ave_log_probas = torch.mean(log_probas)\n",
    "print(ave_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a646ff03-38e5-4ba8-9e35-b9c13c5da886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.0472)\n"
     ]
    }
   ],
   "source": [
    "neg_ave_log_probas = ave_log_probas * -1\n",
    "print(neg_ave_log_probas) # this is called as cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2583af8e-e6ac-4ebd-b09c-02458663ef73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d77e3af0-2903-4900-b854-09ca90aee62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 50257])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits = logits.flatten(0,1)\n",
    "targets = targets.flatten()\n",
    "print(logits.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3233d07e-a8cc-41f7-8e40-1a3430e5cacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.0472)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e672c05-0246-4115-9531-0df853e9ba73",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "=> It is a measure often used alongside croos entropy loss to evaluate the performance of the models. It can provide a more interpretable way to understand the uncertainity of a model in predicting the next token in a sequence\n",
    "\n",
    "=> Lower perplexity means the output predicted output is closer to the actual output.\n",
    "\n",
    "=> perplexity = torch.exp(loss) ex:- it gives 47678 that means the model is not sure which one of the 47678 tokens in the vocab to generate as the next token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f7ca3-b356-48b9-8aee-184844d52643",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4d0630f-119d-4669-af9f-63a51df159ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/siddartha/Desktop/github/llms-from-scratch/Chapter_2_Working_with_text_data/the_verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ec118b5-76d4-4085-b1d7-e2ce547d7477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: \n",
      " 20479\n",
      "Total tokens: \n",
      " 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Total characters: \\n\", total_characters)\n",
    "print(\"Total tokens: \\n\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63e41172-96b4-4904-82e3-954752dddcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f1b148a-3167-4a4d-b405-be5054343aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class GPTDatasetv1(Dataset):\n",
    "    def __init__(self, text, stride, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunks = token_ids[i:i+max_length]\n",
    "            target_chunks = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunks))\n",
    "            self.target_ids.append(torch.tensor(target_chunks))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75d740fe-731c-4d7e-8e2a-d6a057adf39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(text, batch_size, max_length, stride, shuffle=False, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetv1(text=text, max_length=max_length, stride=stride, tokenizer=tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f25db85c-3a54-4200-9e64-438e0f026bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    text=train_data, \n",
    "    batch_size=2, \n",
    "    max_length = GPT_CONFIG_124M[\"context_length\"], \n",
    "    stride = GPT_CONFIG_124M['context_length'],\n",
    "    drop_last=True,\n",
    "    shuffle = True,\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    text = val_data,\n",
    "    batch_size=2,\n",
    "    max_length = GPT_CONFIG_124M['context_length'],\n",
    "    stride=GPT_CONFIG_124M['context_length'],\n",
    "    drop_last = True,\n",
    "    shuffle = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffa7affc-6a85-49af-a196-974875d0e341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader: \n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Loader: \")\n",
    "for x,y in train_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83fd699c-a8bd-4fb4-8075-98c9872de1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loader: \n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Loader: \")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "869f1e20-fb9e-4cf7-bdda-d0952313b195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ecf6fbe-6e0b-451a-a9a1-c8a99d60a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1a4992f-ae85-4242-bc07-2fc7c71a526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to compute the training and validation loss\n",
    "def calc_loss_loader(data_loader, model, device, num_batches = None):\n",
    "    total_loss = 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss+=loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e1aa732-0424-4608-b09d-e880ac621af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: \n",
      " 10.99716133541531\n",
      "Validation Loss: \n",
      " 10.98913860321045\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_loss = calc_loss_loader(train_loader, model, device)\n",
    "val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training Loss: \\n\", train_loss)\n",
    "print(\"Validation Loss: \\n\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601de626-6889-474a-836a-93c7f33a1749",
   "metadata": {},
   "source": [
    "### Typical Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e3d011c-cc32-4ef4-bcc0-ea0afbef02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen+= input_batch.numel()\n",
    "            global_step+=1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"EP {epoch+1} (Step {global_step:06d}):\"\n",
    "                     f\"Train loss {train_loss:3f}, val loss {val_loss:3f}\")\n",
    "        generate_and_print_sample(model, train_loader.dataset.tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "307e4fe1-4e48-46a1-b669-6719dacc5f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7943f31b-8458-476a-9bc4-24787eab4328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_content, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model = model,\n",
    "            idx = encoded,\n",
    "            max_new_tokens = 50,\n",
    "            context_size = context_size,\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\",\" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3bbc9b6-d477-4fa3-ac43-5dbd31b30317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d47b9860-d423-4e1f-a84e-110468c18cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP 1 (Step 000000):Train loss 10.133675, val loss 9.986797\n",
      "EP 1 (Step 000005):Train loss 7.744055, val loss 8.369277\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "EP 2 (Step 000010):Train loss 6.367939, val loss 7.049547\n",
      "EP 2 (Step 000015):Train loss 6.013133, val loss 6.582771\n",
      "Every effort moves you, the, the, the, the, the, the, the. \", the, the, the,, the, the, the, the, the, the, the, the, the, the, the, the, the\n",
      "EP 3 (Step 000020):Train loss 6.008183, val loss 6.537976\n",
      "EP 3 (Step 000025):Train loss 4.547889, val loss 6.366273\n",
      "Every effort moves you, and I had the picture. \"I, and I had been the picture, and I had been the picture, and he was. I had the the picture. I had the picture. I had been the picture. \"I he\n",
      "EP 4 (Step 000030):Train loss 5.118408, val loss 6.342577\n",
      "EP 4 (Step 000035):Train loss 4.470698, val loss 6.319532\n",
      "Every effort moves you knowburn, and I was a--I was his pictures a little. \"I was a little--and here are was a little. \"Oh, I had been. \"I had been, I had been. \"I\n",
      "EP 5 (Step 000040):Train loss 3.223598, val loss 6.347463\n",
      "Every effort moves you know it was not a little a--I was his pictures.   \"Oh, I was--and here are the donkey, and I was his pictures--as I had been. I had the donkey, and it.   \n",
      "EP 6 (Step 000045):Train loss 3.298914, val loss 6.170410\n",
      "EP 6 (Step 000050):Train loss 2.690553, val loss 6.200030\n",
      "Every effort moves you know; and in a little wild--I felt, and I felt in a little: \"Yes, and in the Riv, and Mrs.       \"--I had always _mine_--the, I was his\n",
      "EP 7 (Step 000055):Train loss 2.018878, val loss 6.212560\n",
      "EP 7 (Step 000060):Train loss 1.605311, val loss 6.226195\n",
      "Every effort moves you know; and to have my hostess was--I looked up his painting.                    \"Oh, I, the donkey. \"There were days, his\n",
      "EP 8 (Step 000065):Train loss 1.619787, val loss 6.288810\n",
      "EP 8 (Step 000070):Train loss 0.878447, val loss 6.266222\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a laugh: \"Yes--and by me!\"  He laughed again, and threw back the head to look up at the sketch of the donkey. \"There were days when I\n",
      "EP 9 (Step 000075):Train loss 0.794648, val loss 6.390814\n",
      "EP 9 (Step 000080):Train loss 0.516724, val loss 6.455936\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, I had again run over from the picture--because he's when I\n",
      "EP 10 (Step 000085):Train loss 0.382084, val loss 6.511060\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0004, weight_decay = 0.1)\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq=5, eval_iter=1, start_context=\"Every effort moves you\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "475eb526-1ced-45f5-8879-387883479ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training Losses\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle = \"-.\", label = \"Validation Losses\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc = \"upper right\")\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha = 0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02e629f4-88f9-424e-9b39-e997bc9df415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd70c78-d9e9-45fd-9ee6-5b73e7f15fb3",
   "metadata": {},
   "source": [
    "### Decoding strategies to control randomness\n",
    "1) temperature scaling\n",
    "2) top-k-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed116f52-62a8-45f5-b766-2066fa48ede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d5102dbb-3c93-4f0f-88df-042c789099c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens_ids = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "decode_text = token_ids_to_text(tokens_ids, tokenizer)\n",
    "print(decode_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd03b35-0b51-4eff-a6e9-1edc87c0a8e3",
   "metadata": {},
   "source": [
    "### Temperature Scaling\n",
    "1) it is a technique that adds a probabilistic selection process to the next-token generation task\n",
    "Previously we have used the torch.argmax() to get the highest probability this is called as greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b73d4d96-1c83-4d63-b484-cede3ad384ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0d94647d-7e46-4186-8600-c269af09ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_vocab = {v:k for k,v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0841294d-f24b-41b4-bec4-b926d2f1d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65e5ee57-179b-4006-a07f-1e71b9cdb0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2543b9b-00ac-4b00-9bdb-ad02971ba3b6",
   "metadata": {},
   "source": [
    "The above answer will always be forward because the probability of the logits is more than the other, this is called as the greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "88f44828-b3fd-4c32-9150-ef881b8e8f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "## Replacing the torch.argmax to torch.multinomial\n",
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples = 1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8e87f841-5209-4c5a-84b5-658b3a7a079a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closer x 73\n",
      "every x 0\n",
      "effort x 0\n",
      "forward x 582\n",
      "inches x 2\n",
      "moves x 0\n",
      "pizza x 0\n",
      "toward x 343\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples = 1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{inverse_vocab[i]} x {freq}\")\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514d0df-1af5-454d-99d4-1b8bf1a042e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a994a24f-a53f-43ee-94f2-6196cd7adcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f74e9-c135-4a6c-8052-7f0234bd00d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecb855-e07d-4d33-86f9-9c1834fa871e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ae2cc-d5d4-4524-9f57-50f0f5ecbf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8855f28-33d9-4f66-8d71-9055c4dc1827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad7cfc-d21f-4044-9fc5-c2dc1e996cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af65b3-3020-4a6e-acd5-8d2770635417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a4de3-e3c6-4ead-89da-974580c01d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae97ed8-22bb-447c-bfa6-ac5c377af2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ba646-2edb-434c-908a-148ed597e306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62978e1-20ed-481a-8eb0-4176aec26b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad8388-b11d-4463-9d18-00f105e1f655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336e82a-b18d-4f0f-9f4d-aa3011a40ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca6806-7605-4837-83ad-de9cc7492cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
